import torch
from torch.utils.data import Dataset
import pandas as pd
import numpy as np
import monai.transforms as transforms
from monai.data import ThreadDataLoader
import os
from torch.nn.utils.rnn import pad_sequence

# get_transforms å‡½æ•°ä¿æŒä¸å˜
def get_transforms(mode, image_size):
    image_key = ["image"]
    load_and_ensure_channel = [
        transforms.LoadImaged(keys=image_key, image_only=True, ensure_channel_first=True),
    ]
    # <<< æ”¹åŠ¨ 1: éªŒè¯é›†å’Œæµ‹è¯•é›†éƒ½ä¸åº”ä½¿ç”¨æ•°æ®å¢å¼º >>>
    # åŸé€»è¾‘åªæœ‰ train å’Œ elseï¼Œç°åœ¨æ˜ç¡® val å’Œ test éƒ½ä¸å¢å¼º
    if mode == 'train':
        data_augmentation = [
            transforms.RandFlipd(keys=image_key, prob=0.5, spatial_axis=0),
            transforms.RandFlipd(keys=image_key, prob=0.5, spatial_axis=1),
            transforms.RandFlipd(keys=image_key, prob=0.5, spatial_axis=2),
        ]
    else: # mode is 'val' or 'test'
        data_augmentation = []
        
    post_processing = [
        transforms.Resized(keys=image_key, spatial_size=image_size, mode="trilinear"),
        transforms.ScaleIntensityRanged(keys=image_key, a_min=-1000.0, a_max=1000.0, b_min=0.0, b_max=1.0, clip=True),
        transforms.ToTensord(keys=image_key) # ç¡®ä¿è¾“å‡ºæ˜¯Tensor
    ]
    return transforms.Compose(load_and_ensure_channel + data_augmentation + post_processing)

# <<< æ”¹åŠ¨ 2: SurvivalDataset ç±»ä¿®æ”¹ >>>
class SurvivalDataset(Dataset):
    # fold_idx å‚æ•°ä¸å†éœ€è¦ï¼Œå·²ç§»é™¤
    def __init__(self, csv_path, mode, image_size, time_points: np.ndarray):
        # å…è®¸ mode ä¸º 'train', 'val', 'test'
        if mode not in ['train', 'val', 'test']: 
            raise ValueError("mode å¿…é¡»æ˜¯ 'train', 'val' æˆ– 'test'")
        self.mode, self.image_size = mode, image_size
        
        self.time_points = torch.tensor(time_points, dtype=torch.float32)
        self.num_multiclass = len(time_points)
        
        # å°è¯•ç”¨æ›´å¥å£®çš„æ–¹å¼è¯»å–CSV
        try:
            full_df = pd.read_csv(csv_path, encoding='utf-8')
        except UnicodeDecodeError:
            full_df = pd.read_csv(csv_path, encoding='gbk')
        
        # æ ¸å¿ƒæ”¹åŠ¨ï¼šæ ¹æ® 'split' åˆ—ç­›é€‰æ•°æ®ï¼Œè€Œä¸æ˜¯ 'k' åˆ—
        if 'split' not in full_df.columns:
            raise ValueError(f"é”™è¯¯: CSVæ–‡ä»¶ '{csv_path}' ä¸­æœªæ‰¾åˆ° 'split' åˆ—ã€‚è¯·å…ˆè¿è¡Œæ•°æ®åˆ’åˆ†è„šæœ¬ã€‚")
        
        self.df = full_df[full_df['split_std'] == self.mode].reset_index(drop=True)
            
        print(f"--- {self.mode.capitalize()} Dataset Initialized ---")
        # ç§»é™¤äº†æ‰“å° Fold çš„ä¿¡æ¯
        print(f"Mode: {self.mode}, Target Time Points: {time_points.tolist()}, Samples: {len(self.df)}")
        self.transform = get_transforms(mode=self.mode, image_size=self.image_size)

    def __len__(self): return len(self.df)

    def __getitem__(self, idx):
        # __getitem__ å†…éƒ¨é€»è¾‘å®Œå…¨ä¸å˜ï¼Œå› ä¸ºå®ƒåªå…³å¿ƒ self.df çš„å†…å®¹
        patient_row = self.df.loc[idx]
        base_filepath = patient_row['output_filepath']
        patient_dir = os.path.dirname(base_filepath)
        
        image_paths = []
        if os.path.exists(patient_dir):
            for filename in sorted(os.listdir(patient_dir)):
                if '.nii' in filename or '.gz' in filename:
                    image_paths.append(os.path.join(patient_dir, filename))
        
        image_tensors_list = [self.transform({'image': path})['image'] for path in image_paths]

        if image_tensors_list:
            images_tensor = torch.stack(image_tensors_list, dim=0)
            if images_tensor.shape[1] == 1:
                images_tensor = images_tensor.repeat(1, 3, 1, 1, 1)
        else:
            images_tensor = torch.empty((0, 3, *self.image_size), dtype=torch.float32)

        time, event = patient_row['PFS'], patient_row['is_progress']
        
        multiclass_labels = torch.zeros(self.num_multiclass, dtype=torch.float32)
        multiclass_masks = torch.zeros(self.num_multiclass, dtype=torch.float32)
        
        for i, t_point in enumerate(self.time_points):
            if time >= t_point:
                multiclass_labels[i] = 1.0
                multiclass_masks[i] = 1.0
            else:
                if event == 1:
                    multiclass_labels[i] = 0.0
                    multiclass_masks[i] = 1.0
                else:
                    multiclass_labels[i] = 0.0
                    multiclass_masks[i] = 0.0

        return {
            "images": images_tensor,
            "time": torch.tensor(time, dtype=torch.float32),
            "event": torch.tensor(event, dtype=torch.float32),
            "multiclass_labels": multiclass_labels,
            "multiclass_masks": multiclass_masks
        }

# custom_collate_fn å‡½æ•°ä¿æŒä¸å˜
def custom_collate_fn(batch):
    image_sequences = [item['images'] for item in batch]
    lengths = torch.tensor([seq.shape[0] for seq in image_sequences], dtype=torch.int64)
    
    padded_images = pad_sequence(image_sequences, batch_first=True, padding_value=0)
    
    times = torch.stack([item['time'] for item in batch])
    events = torch.stack([item['event'] for item in batch])
    multiclass_labels = torch.stack([item['multiclass_labels'] for item in batch])
    multiclass_masks = torch.stack([item['multiclass_masks'] for item in batch])

    return {
        'images': padded_images,
        'lengths': lengths,
        'time': times,
        'event': events,
        'multiclass_labels': multiclass_labels,
        'multiclass_masks': multiclass_masks
    }

# <<< æ”¹åŠ¨ 3: get_dataloaders å‡½æ•°å¤§æ”¹ >>>
def get_dataloaders(csv_path, image_size, time_points, batch_size=4, num_workers=4):
    """
    æ ¹æ® 'split' åˆ—åˆ›å»ºè®­ç»ƒã€éªŒè¯å’Œæµ‹è¯•é›†çš„ DataLoaderã€‚
    """
    # å®ä¾‹åŒ–ä¸‰ä¸ªæ•°æ®é›†
    train_dataset = SurvivalDataset(
        csv_path=csv_path, mode='train', image_size=image_size, time_points=time_points
    )
    val_dataset = SurvivalDataset(
        csv_path=csv_path, mode='test', image_size=image_size, time_points=time_points
    )
    test_dataset = SurvivalDataset(
        csv_path=csv_path, mode='val', image_size=image_size, time_points=time_points
    )
    
    print("\n--- Creating Train/Val/Test DataLoaders ---")
    
    # åˆ›å»ºä¸‰ä¸ª DataLoader
    train_loader = ThreadDataLoader(
        train_dataset, batch_size=batch_size, shuffle=True, 
        num_workers=num_workers, collate_fn=custom_collate_fn, pin_memory=True
    )
    val_loader = ThreadDataLoader(
        val_dataset, batch_size=batch_size, shuffle=False, 
        num_workers=num_workers, collate_fn=custom_collate_fn, pin_memory=True
    )
    test_loader = ThreadDataLoader(
        test_dataset, batch_size=batch_size, shuffle=False, 
        num_workers=num_workers, collate_fn=custom_collate_fn, pin_memory=True
    )
    
    # è¿”å›ä¸€ä¸ªåŒ…å«ä¸‰ä¸ª loader çš„å­—å…¸
    return {'train': train_loader, 'val': val_loader, 'test': test_loader}

# <<< æ”¹åŠ¨ 4: å…¨æ–°ã€åŠŸèƒ½æ›´å…¨é¢çš„æµ‹è¯•è„šæœ¬ >>>
if __name__ == '__main__':
    # å‡è®¾ä½ å·²ç»è¿è¡Œäº† prepare_dataset_split.py å¹¶ç”Ÿæˆäº†è¿™ä¸ªæ–‡ä»¶
    # !! è¯·ç¡®ä¿è¿™ä¸ªè·¯å¾„æ˜¯æ­£ç¡®çš„ !!
    CSV_FILE_PATH = r"/data/yuanjiahong/yhh/code/utils_1024/id_adress_pfs_kfolder_v4_testeasy811.csv"

    MODEL_IMAGE_SIZE = (48, 256, 256) 
    BATCH_SIZE = 4
    FIVE_TIME_POINTS = np.array([6.0, 8.0, 10.0, 12.0, 16.0])
    NUM_TIME_POINTS = len(FIVE_TIME_POINTS)

    # --- æ¨¡æ‹Ÿæ•°æ®ç”Ÿæˆ ---
    if not os.path.exists(CSV_FILE_PATH):
        print(f"\n[è­¦å‘Š] æœªæ‰¾åˆ°CSVæ–‡ä»¶ '{CSV_FILE_PATH}'ã€‚å°†åˆ›å»ºä¸€ä¸ªæ¨¡æ‹ŸCSVæ–‡ä»¶ç”¨äºè°ƒè¯•ã€‚")
        dummy_data_dir = "./dummy_patient_data"
        os.makedirs(dummy_data_dir, exist_ok=True)
        dummy_nii_path = os.path.join(dummy_data_dir, "scan1.nii.gz")
        if not os.path.exists(dummy_nii_path):
            import SimpleITK as sitk
            dummy_image = sitk.GetImageFromArray(np.zeros(MODEL_IMAGE_SIZE, dtype=np.int16))
            sitk.WriteImage(dummy_image, dummy_nii_path)

        dummy_data = {
            'output_filepath': [dummy_nii_path] * 10,
            'PFS': [5.0, 7.0, 9.5, 11.0, 15.0, 18.0, 13.0, 20.0, 4.0, 22.0],
            'is_progress': [1, 1, 1, 0, 1, 1, 0, 0, 1, 0],
            'k': [0]*10,
            'split': ['train']*7 + ['val']*1 + ['test']*2
        }
        pd.DataFrame(dummy_data).to_csv(CSV_FILE_PATH, index=False)
        print(f"æ¨¡æ‹ŸCSVæ–‡ä»¶å·²åˆ›å»ºåœ¨ '{CSV_FILE_PATH}'\n")

    print("="*70)
    print(" " * 15 + "DETAILED TEST: Train/Val/Test Data Pipeline")
    print("="*70)
    
    try:
        # --- 1. åˆ›å»º DataLoaders ---
        dataloaders = get_dataloaders(
            csv_path=CSV_FILE_PATH,
            image_size=MODEL_IMAGE_SIZE,
            time_points=FIVE_TIME_POINTS,
            batch_size=BATCH_SIZE,
            num_workers=0 
        )
        
        # --- 2. éªŒè¯ DataLoaders ç»“æ„ ---
        print("\n--- [æ£€æŸ¥ 1] DataLoader ç»“æ„éªŒè¯ ---")
        expected_keys = ['train', 'val', 'test']
        print(f" > è¿”å›çš„å­—å…¸åŒ…å«çš„é”®: {list(dataloaders.keys())}")
        assert all(k in dataloaders for k in expected_keys), "é”™è¯¯: å­—å…¸ä¸­ç¼ºå°‘å¿…è¦çš„é”®ï¼"
        print(" > ç»“æ„éªŒè¯é€šè¿‡ï¼\n")

        # --- 3. é€ä¸€æ£€æŸ¥æ¯ä¸ª DataLoader ---
        full_df = pd.read_csv(CSV_FILE_PATH)
        for split_name, loader in dataloaders.items():
            print("-" * 70)
            #print(f"--- [æ£€æŸ¥] è¯¦ç»†åˆ†æ <{split_name.toUpperCase()}> DataLoader ---")
            
            expected_samples = len(full_df[full_df['split_std'] == split_name])
            actual_samples = len(loader.dataset)
            print(f" > æ ·æœ¬æ•°: é¢„æœŸ(CSVä¸­)={expected_samples}, å®é™…(Datasetä¸­)={actual_samples}")
            assert expected_samples == actual_samples, "é”™è¯¯: æ ·æœ¬æ•°ä¸åŒ¹é…ï¼"

            if not loader:
                print(" > Loader ä¸ºç©ºï¼Œè·³è¿‡åç»­æ£€æŸ¥ã€‚")
                continue

            batch_data = next(iter(loader))
            print(" > æˆåŠŸè·å–ä¸€ä¸ªæ•°æ®æ‰¹æ¬¡ï¼")

            print("\n   --- Batch å†…å®¹è¯¦ç»†ä¿¡æ¯ ---")
            current_batch_size = batch_data['images'].shape[0]
            for key, tensor in batch_data.items():
                print(f"   - {key:<18}: shape={list(tensor.shape)}, dtype={tensor.dtype}, device={tensor.device}")
            
            # <<< æ ¸å¿ƒä¿®æ­£: ä» [1:] æ”¹ä¸º [2:]ï¼Œä»¥é€‚åº”å˜é•¿çš„åºåˆ— >>>
            # è¿™ä¸ªæ–­è¨€æ£€æŸ¥çš„æ˜¯(é€šé“æ•°, æ·±åº¦, é«˜åº¦, å®½åº¦)
            assert batch_data['images'].shape[2:] == (3, *MODEL_IMAGE_SIZE), "å½±åƒå¼ é‡å½¢çŠ¶é”™è¯¯"
            
            assert len(batch_data['lengths']) == current_batch_size, "lengths é•¿åº¦é”™è¯¯"
            assert batch_data['multiclass_labels'].shape == (current_batch_size, NUM_TIME_POINTS), "æ ‡ç­¾å½¢çŠ¶é”™è¯¯"
            assert batch_data['multiclass_masks'].shape == (current_batch_size, NUM_TIME_POINTS), "æ©ç å½¢çŠ¶é”™è¯¯"
            print("   > æ‰€æœ‰å¼ é‡å½¢çŠ¶ç¬¦åˆé¢„æœŸï¼")

            print("\n   --- ç¬¬ä¸€ä¸ªæ ·æœ¬å†…å®¹æŠ½æ ·æ£€æŸ¥ ---")
            idx = 0
            time_sample = batch_data['time'][idx].item()
            event_sample = batch_data['event'][idx].item()
            labels_sample = batch_data['multiclass_labels'][idx].tolist()
            masks_sample = batch_data['multiclass_masks'][idx].tolist()

            print(f"   > åŸå§‹æ•°æ®: PFS={time_sample:.2f} æœˆ, Event={'å‘ç”Ÿ' if event_sample == 1 else 'åˆ å¤±'}")
            print(f"   > ç›®æ ‡æ—¶é—´ç‚¹: {FIVE_TIME_POINTS.tolist()}")
            print(f"   > ç”Ÿæˆçš„æ ‡ç­¾: {labels_sample}")
            print(f"   > ç”Ÿæˆçš„æ©ç : {masks_sample}")
            
            expected_labels, expected_masks = [], []
            for t_point in FIVE_TIME_POINTS:
                if time_sample >= t_point:
                    expected_labels.append(1.0); expected_masks.append(1.0)
                else:
                    if event_sample == 1:
                        expected_labels.append(0.0); expected_masks.append(1.0)
                    else:
                        expected_labels.append(0.0); expected_masks.append(0.0)
            assert labels_sample == expected_labels, "é”™è¯¯: æ ‡ç­¾ç”Ÿæˆé€»è¾‘ä¸åŒ¹é…ï¼"
            assert masks_sample == expected_masks, "é”™è¯¯: æ©ç ç”Ÿæˆé€»è¾‘ä¸åŒ¹é…ï¼"
            print("   > æŠ½æ ·æ£€æŸ¥é€šè¿‡ï¼Œæ ‡ç­¾å’Œæ©ç ç”Ÿæˆé€»è¾‘æ­£ç¡®ï¼")
        
        print("\n" + "="*70)
        print(" " * 10 + "ğŸ‰ Train/Val/Test æ•°æ®ç®¡é“å‡çº§æˆåŠŸï¼Œæ‰€æœ‰è¯¦ç»†æ£€æŸ¥é€šè¿‡ï¼ ğŸ‰")
        print("="*70)

    except Exception as e:
        import traceback
        traceback.print_exc()
        print(f"\n[é”™è¯¯] æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸: {e}")